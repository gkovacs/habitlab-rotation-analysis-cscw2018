{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import lzstring\n",
    "from collections import namedtuple, Counter\n",
    "import json\n",
    "from memoize import memoize\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [num unique urls, num unique urls typed, total visits, total typed, first visit time, last visit time]\n",
    "domaininfo = namedtuple('domaininfo', ['num_unique_urls', 'num_unique_urls_typed', 'total_visits', 'total_typed', 'first_visit_time', 'last_visit_time'])\n",
    "\n",
    "decompressFromEncodedURIComponent = lzstring.LZString().decompressFromEncodedURIComponent\n",
    "\n",
    "filepath = 'difficultyselectionexp_may31_11am.csv'\n",
    "reader = csv.DictReader(open(filepath))\n",
    "\n",
    "def extract_domain_visit_info(domain_visit_info_compressed):\n",
    "  domain_visit_info = json.loads(decompressFromEncodedURIComponent(domain_visit_info_compressed))\n",
    "  output = {}\n",
    "  for k,v in domain_visit_info.items():\n",
    "    linedata = domaininfo(*v)\n",
    "    output[k] = linedata\n",
    "  return output\n",
    "\n",
    "alldata = []\n",
    "\n",
    "for alldata_item in reader:\n",
    "  if alldata_item['selected_difficulty'] not in ['nothing', 'easy', 'medium', 'hard']:\n",
    "    continue\n",
    "  if alldata_item['domain_visit_info_compressed'] == None or len(alldata_item['domain_visit_info_compressed']) == 0:\n",
    "    continue\n",
    "  alldata_item['domain_visit_info'] = extract_domain_visit_info(alldata_item['domain_visit_info_compressed'])\n",
    "  alldata.append(alldata_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "training_data = alldata[:round(len(alldata)*0.8)]\n",
    "test_data = alldata[round(len(alldata)*0.8):]\n",
    "print(len(training_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_alldata(data):\n",
    "  return np.array([line['selected_difficulty'] for line in data])\n",
    "\n",
    "@memoize\n",
    "def get_most_common_label():\n",
    "  label_to_count = Counter()\n",
    "  for line in training_data:\n",
    "    label = line['selected_difficulty']\n",
    "    label_to_count[label] += 1\n",
    "  sorted_by_count = sorted(label_to_count.items(), key=lambda x: x[1], reverse=True)\n",
    "  return sorted_by_count[0][0]\n",
    "\n",
    "@memoize\n",
    "def get_most_visited_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += info.total_visits\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "@memoize\n",
    "def get_most_common_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += 1\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "def get_num_visits_for_domain(domain_visit_info, domain):\n",
    "  info = domain_visit_info.get(domain, None)\n",
    "  if info != None:\n",
    "    return info.total_visits\n",
    "  return 0\n",
    "\n",
    "def extract_features_for_user(domain_visit_info):\n",
    "  domains = get_most_common_domains()\n",
    "  visits_for_domains = np.array([get_num_visits_for_domain(domain_visit_info, x) for x in domains])\n",
    "  visits_for_domains = np.divide(visits_for_domains, np.sum(visits_for_domains))\n",
    "  return visits_for_domains\n",
    "\n",
    "def extract_features_alldata(data):\n",
    "  output = []\n",
    "  for line in data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    features = extract_features_for_user(domain_visit_info)\n",
    "    output.append(features)\n",
    "  return np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline classifier accuracy: 0.48\n"
     ]
    }
   ],
   "source": [
    "def get_percent_correct(predicted_labels, actual_labels):\n",
    "  if len(predicted_labels) != len(actual_labels):\n",
    "    raise 'need predicted and actual labels to have same lengths'\n",
    "  total = len(actual_labels)\n",
    "  correct = 0\n",
    "  for p,a in zip(predicted_labels, actual_labels):\n",
    "    if p == a:\n",
    "      correct += 1\n",
    "  return correct / total\n",
    "\n",
    "def test_baseline_classifier():\n",
    "  most_common_label = get_most_common_label()\n",
    "  predictions = [most_common_label for line in test_data]\n",
    "  actual = extract_labels_alldata(test_data)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print('baseline classifier accuracy:', percent_correct)\n",
    "\n",
    "def test_classifier(clf, str=None):\n",
    "  actual = extract_labels_alldata(test_data)\n",
    "  features_test = extract_features_alldata(test_data)\n",
    "  predictions = clf.predict(features_test)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print(str + ' classifier testing accuracy:', percent_correct)\n",
    "\n",
    "def training_error_classifier(clf, str=None):\n",
    "  actual = extract_labels_alldata(training_data)\n",
    "  features_train = extract_features_alldata(training_data)\n",
    "  predictions = clf.predict(features_train)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print(str + ' classifier training accuracy:', round(percent_correct, 2))\n",
    "\n",
    "def to_int_categorical(dt):\n",
    "  # {'easy', 'hard', 'medium', 'nothing'}\n",
    "  cat_dt = []\n",
    "  for item in dt:\n",
    "    if item == 'nothing':\n",
    "      cat_dt.append(0)\n",
    "    elif item == 'easy':\n",
    "      cat_dt.append(1)\n",
    "    elif item == 'medium':\n",
    "      cat_dt.append(2)\n",
    "    else:\n",
    "      cat_dt.append(3)\n",
    "  return np.array(cat_dt)\n",
    "    \n",
    "test_baseline_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF classifier testing accuracy: 0.28\n",
      "KNN classifier testing accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "labels_train = extract_labels_alldata(training_data)\n",
    "features_train = extract_features_alldata(training_data)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, 'RF')\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, p=1)\n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 1.3985 - acc: 0.2245\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 0s 55us/step - loss: 1.3865 - acc: 0.2653\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 0s 69us/step - loss: 1.3676 - acc: 0.2959\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 0s 61us/step - loss: 1.3452 - acc: 0.3980\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 0s 85us/step - loss: 1.3555 - acc: 0.3878\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 0s 64us/step - loss: 1.3245 - acc: 0.4184\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 0s 82us/step - loss: 1.3143 - acc: 0.4082\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 0s 74us/step - loss: 1.2957 - acc: 0.4184\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 0s 80us/step - loss: 1.3050 - acc: 0.3980\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 0s 75us/step - loss: 1.2938 - acc: 0.4286\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 0s 86us/step - loss: 1.2937 - acc: 0.4082\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 0s 104us/step - loss: 1.3030 - acc: 0.4082\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 0s 87us/step - loss: 1.2908 - acc: 0.4184\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 0s 81us/step - loss: 1.2906 - acc: 0.4286\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 0s 80us/step - loss: 1.2827 - acc: 0.4082\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 0s 78us/step - loss: 1.3176 - acc: 0.3469\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 0s 79us/step - loss: 1.2981 - acc: 0.3980\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 0s 74us/step - loss: 1.2957 - acc: 0.4184\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 0s 82us/step - loss: 1.2904 - acc: 0.4388\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 0s 75us/step - loss: 1.3025 - acc: 0.4184\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - 0s 107us/step - loss: 1.3169 - acc: 0.4082\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - 0s 87us/step - loss: 1.3073 - acc: 0.4082\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - 0s 78us/step - loss: 1.3012 - acc: 0.4184\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - 0s 92us/step - loss: 1.2988 - acc: 0.4082\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - 0s 88us/step - loss: 1.2919 - acc: 0.4286\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - 0s 92us/step - loss: 1.2652 - acc: 0.4490\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - 0s 110us/step - loss: 1.2891 - acc: 0.4286\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - 0s 103us/step - loss: 1.2917 - acc: 0.4184\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - 0s 101us/step - loss: 1.2893 - acc: 0.4082\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - 0s 104us/step - loss: 1.2855 - acc: 0.4184\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - 0s 96us/step - loss: 1.3024 - acc: 0.4184\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - 0s 102us/step - loss: 1.3079 - acc: 0.4286\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - 0s 106us/step - loss: 1.2947 - acc: 0.4082\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - 0s 88us/step - loss: 1.2927 - acc: 0.4082\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - 0s 98us/step - loss: 1.2771 - acc: 0.4082\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - 0s 83us/step - loss: 1.2906 - acc: 0.4082\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - 0s 90us/step - loss: 1.2938 - acc: 0.4286\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - 0s 80us/step - loss: 1.2869 - acc: 0.4184\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - 0s 92us/step - loss: 1.2661 - acc: 0.4490\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - 0s 85us/step - loss: 1.2854 - acc: 0.4184\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - 0s 70us/step - loss: 1.2890 - acc: 0.3776\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - 0s 97us/step - loss: 1.2795 - acc: 0.4286\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - 0s 74us/step - loss: 1.2895 - acc: 0.4184\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - 0s 95us/step - loss: 1.2979 - acc: 0.4082\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - 0s 76us/step - loss: 1.2882 - acc: 0.4388\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - 0s 96us/step - loss: 1.2861 - acc: 0.4184\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - 0s 79us/step - loss: 1.2717 - acc: 0.4184\n",
      "Epoch 48/100\n",
      "98/98 [==============================] - 0s 71us/step - loss: 1.2915 - acc: 0.4082\n",
      "Epoch 49/100\n",
      "98/98 [==============================] - 0s 112us/step - loss: 1.2921 - acc: 0.4082\n",
      "Epoch 50/100\n",
      "98/98 [==============================] - 0s 83us/step - loss: 1.2831 - acc: 0.4082\n",
      "Epoch 51/100\n",
      "98/98 [==============================] - 0s 80us/step - loss: 1.2924 - acc: 0.4184\n",
      "Epoch 52/100\n",
      "98/98 [==============================] - 0s 88us/step - loss: 1.2672 - acc: 0.4286\n",
      "Epoch 53/100\n",
      "98/98 [==============================] - 0s 67us/step - loss: 1.2696 - acc: 0.4388\n",
      "Epoch 54/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3591 - acc: 0.343 - 0s 77us/step - loss: 1.2813 - acc: 0.4286\n",
      "Epoch 55/100\n",
      "98/98 [==============================] - 0s 98us/step - loss: 1.2970 - acc: 0.4184\n",
      "Epoch 56/100\n",
      "98/98 [==============================] - 0s 77us/step - loss: 1.2834 - acc: 0.3980\n",
      "Epoch 57/100\n",
      "98/98 [==============================] - 0s 69us/step - loss: 1.2783 - acc: 0.4184\n",
      "Epoch 58/100\n",
      "98/98 [==============================] - 0s 94us/step - loss: 1.2796 - acc: 0.4286\n",
      "Epoch 59/100\n",
      "98/98 [==============================] - 0s 90us/step - loss: 1.2790 - acc: 0.4388\n",
      "Epoch 60/100\n",
      "98/98 [==============================] - 0s 76us/step - loss: 1.2785 - acc: 0.4184\n",
      "Epoch 61/100\n",
      "98/98 [==============================] - 0s 71us/step - loss: 1.2993 - acc: 0.4082\n",
      "Epoch 62/100\n",
      "98/98 [==============================] - 0s 72us/step - loss: 1.2812 - acc: 0.4082\n",
      "Epoch 63/100\n",
      "98/98 [==============================] - 0s 101us/step - loss: 1.2719 - acc: 0.4082\n",
      "Epoch 64/100\n",
      "98/98 [==============================] - 0s 102us/step - loss: 1.2968 - acc: 0.4082\n",
      "Epoch 65/100\n",
      "98/98 [==============================] - 0s 88us/step - loss: 1.2988 - acc: 0.4490\n",
      "Epoch 66/100\n",
      "98/98 [==============================] - 0s 76us/step - loss: 1.2912 - acc: 0.4082\n",
      "Epoch 67/100\n",
      "98/98 [==============================] - 0s 100us/step - loss: 1.2789 - acc: 0.4286\n",
      "Epoch 68/100\n",
      "98/98 [==============================] - 0s 89us/step - loss: 1.2888 - acc: 0.3776\n",
      "Epoch 69/100\n",
      "98/98 [==============================] - 0s 75us/step - loss: 1.2764 - acc: 0.4694\n",
      "Epoch 70/100\n",
      "98/98 [==============================] - 0s 87us/step - loss: 1.2958 - acc: 0.3980\n",
      "Epoch 71/100\n",
      "98/98 [==============================] - 0s 103us/step - loss: 1.3082 - acc: 0.3673\n",
      "Epoch 72/100\n",
      "98/98 [==============================] - 0s 92us/step - loss: 1.2743 - acc: 0.4388\n",
      "Epoch 73/100\n",
      "98/98 [==============================] - 0s 80us/step - loss: 1.2875 - acc: 0.3980\n",
      "Epoch 74/100\n",
      "98/98 [==============================] - 0s 83us/step - loss: 1.2842 - acc: 0.3776\n",
      "Epoch 75/100\n",
      "98/98 [==============================] - 0s 106us/step - loss: 1.2827 - acc: 0.4184\n",
      "Epoch 76/100\n",
      "98/98 [==============================] - 0s 91us/step - loss: 1.2814 - acc: 0.4184\n",
      "Epoch 77/100\n",
      "98/98 [==============================] - 0s 86us/step - loss: 1.2572 - acc: 0.4286\n",
      "Epoch 78/100\n",
      "98/98 [==============================] - 0s 66us/step - loss: 1.2684 - acc: 0.4184\n",
      "Epoch 79/100\n",
      "98/98 [==============================] - 0s 101us/step - loss: 1.2981 - acc: 0.4082\n",
      "Epoch 80/100\n",
      "98/98 [==============================] - 0s 77us/step - loss: 1.2826 - acc: 0.4184\n",
      "Epoch 81/100\n",
      "98/98 [==============================] - 0s 110us/step - loss: 1.2626 - acc: 0.4490\n",
      "Epoch 82/100\n",
      "98/98 [==============================] - 0s 95us/step - loss: 1.2720 - acc: 0.4184\n",
      "Epoch 83/100\n",
      "98/98 [==============================] - 0s 116us/step - loss: 1.2927 - acc: 0.4286\n",
      "Epoch 84/100\n",
      "98/98 [==============================] - 0s 67us/step - loss: 1.2576 - acc: 0.4184\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 0s 93us/step - loss: 1.2903 - acc: 0.4184\n",
      "Epoch 86/100\n",
      "98/98 [==============================] - 0s 110us/step - loss: 1.2841 - acc: 0.4286\n",
      "Epoch 87/100\n",
      "98/98 [==============================] - 0s 76us/step - loss: 1.2974 - acc: 0.4184\n",
      "Epoch 88/100\n",
      "98/98 [==============================] - 0s 73us/step - loss: 1.2811 - acc: 0.4184\n",
      "Epoch 89/100\n",
      "98/98 [==============================] - 0s 83us/step - loss: 1.2856 - acc: 0.4184\n",
      "Epoch 90/100\n",
      "98/98 [==============================] - 0s 84us/step - loss: 1.2887 - acc: 0.4082\n",
      "Epoch 91/100\n",
      "98/98 [==============================] - 0s 104us/step - loss: 1.2841 - acc: 0.4184\n",
      "Epoch 92/100\n",
      "98/98 [==============================] - 0s 94us/step - loss: 1.2904 - acc: 0.4184\n",
      "Epoch 93/100\n",
      "98/98 [==============================] - 0s 111us/step - loss: 1.3071 - acc: 0.4184\n",
      "Epoch 94/100\n",
      "98/98 [==============================] - 0s 90us/step - loss: 1.2621 - acc: 0.4184\n",
      "Epoch 95/100\n",
      "98/98 [==============================] - 0s 94us/step - loss: 1.2782 - acc: 0.4184\n",
      "Epoch 96/100\n",
      "98/98 [==============================] - 0s 88us/step - loss: 1.2827 - acc: 0.4184\n",
      "Epoch 97/100\n",
      "98/98 [==============================] - 0s 84us/step - loss: 1.2828 - acc: 0.4082\n",
      "Epoch 98/100\n",
      "98/98 [==============================] - 0s 87us/step - loss: 1.2915 - acc: 0.4184\n",
      "Epoch 99/100\n",
      "98/98 [==============================] - 0s 113us/step - loss: 1.2765 - acc: 0.4286\n",
      "Epoch 100/100\n",
      "98/98 [==============================] - 0s 150us/step - loss: 1.2594 - acc: 0.4592\n",
      "25/25 [==============================] - 0s 7ms/step\n",
      "[[0.13518189 0.4027941  0.30851406 0.15351002]\n",
      " [0.12950128 0.41416505 0.30635685 0.14997686]\n",
      " [0.1181064  0.42436808 0.3165178  0.14100772]\n",
      " [0.13485605 0.37052408 0.3439063  0.15071358]\n",
      " [0.14585216 0.40059155 0.29813284 0.15542342]\n",
      " [0.14304279 0.39839083 0.30693343 0.151633  ]\n",
      " [0.14724003 0.40399182 0.29904684 0.1497213 ]\n",
      " [0.13027899 0.41982764 0.30673265 0.14316078]\n",
      " [0.12283958 0.41455975 0.31996444 0.14263627]\n",
      " [0.14134704 0.41216597 0.29689404 0.14959292]\n",
      " [0.11970637 0.4230268  0.3190661  0.13820074]\n",
      " [0.11668485 0.40624964 0.33055016 0.14651535]\n",
      " [0.12577368 0.4556859  0.27261105 0.14592943]\n",
      " [0.12366312 0.40034172 0.32930565 0.14668947]\n",
      " [0.12895568 0.38927123 0.32533467 0.1564384 ]\n",
      " [0.13046385 0.36822426 0.34985265 0.15145926]\n",
      " [0.11497013 0.41743448 0.32768473 0.13991064]\n",
      " [0.13050276 0.4427353  0.29961932 0.12714258]\n",
      " [0.11588766 0.4118605  0.33169731 0.14055449]\n",
      " [0.11904365 0.42976356 0.3121546  0.13903825]\n",
      " [0.13686356 0.3979208  0.32061106 0.14460462]\n",
      " [0.13803335 0.4316259  0.28190798 0.1484327 ]\n",
      " [0.12039861 0.4189184  0.3188016  0.14188142]\n",
      " [0.13504954 0.39663285 0.31166863 0.15664887]\n",
      " [0.12246484 0.43829158 0.3026999  0.13654374]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[1.2018253803253174, 0.47999998927116394]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "labels_test = to_categorical(to_int_categorical(extract_labels_alldata(test_data)), num_classes=4)\n",
    "features_test = extract_features_alldata(test_data)\n",
    "\n",
    "labels_train = to_categorical(to_int_categorical(extract_labels_alldata(training_data)), num_classes=4)\n",
    "features_train = extract_features_alldata(training_data)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(features_train, labels_train,\n",
    "          epochs=100,\n",
    "          batch_size=32)\n",
    "score = model.evaluate(features_test, labels_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(features_test, batch_size=32)\n",
    "\n",
    "print(predictions)\n",
    "print(labels_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
