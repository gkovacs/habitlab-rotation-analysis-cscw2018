{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import lzstring\n",
    "from collections import namedtuple, Counter\n",
    "import json\n",
    "from memoize import memoize\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [num unique urls, num unique urls typed, total visits, total typed, first visit time, last visit time]\n",
    "domaininfo = namedtuple('domaininfo', ['num_unique_urls', 'num_unique_urls_typed', 'total_visits', 'total_typed', 'first_visit_time', 'last_visit_time'])\n",
    "\n",
    "decompressFromEncodedURIComponent = lzstring.LZString().decompressFromEncodedURIComponent\n",
    "\n",
    "#filepath = 'difficultyselectionexp_may31_11am.csv'\n",
    "filepath = 'difficultyselectionexp_june25_9pm.csv'\n",
    "reader = csv.DictReader(open(filepath))\n",
    "\n",
    "def extract_domain_visit_info(domain_visit_info_compressed):\n",
    "  domain_visit_info = json.loads(decompressFromEncodedURIComponent(domain_visit_info_compressed))\n",
    "  output = {}\n",
    "  for k,v in domain_visit_info.items():\n",
    "    linedata = domaininfo(*v)\n",
    "    output[k] = linedata\n",
    "  return output\n",
    "\n",
    "alldata = []\n",
    "\n",
    "for alldata_item in reader:\n",
    "  if alldata_item['selected_difficulty'] not in ['nothing', 'easy', 'medium', 'hard']:\n",
    "    continue\n",
    "  if alldata_item['domain_visit_info_compressed'] == None or len(alldata_item['domain_visit_info_compressed']) == 0:\n",
    "    continue\n",
    "  alldata_item['domain_visit_info'] = extract_domain_visit_info(alldata_item['domain_visit_info_compressed'])\n",
    "  alldata.append(alldata_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = alldata[:round(len(alldata)*0.8)]\n",
    "test_data = alldata[round(len(alldata)*0.8):]\n",
    "print(len(training_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_alldata(data):\n",
    "  return np.array([line['selected_difficulty'] for line in data])\n",
    "\n",
    "@memoize\n",
    "def get_most_common_label():\n",
    "  label_to_count = Counter()\n",
    "  for line in training_data:\n",
    "    label = line['selected_difficulty']\n",
    "    label_to_count[label] += 1\n",
    "  sorted_by_count = sorted(label_to_count.items(), key=lambda x: x[1], reverse=True)\n",
    "  return sorted_by_count[0][0]\n",
    "\n",
    "@memoize\n",
    "def get_most_visited_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += info.total_visits\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "@memoize\n",
    "def get_most_common_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += 1\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "def get_num_visits_for_domain(domain_visit_info, domain):\n",
    "  info = domain_visit_info.get(domain, None)\n",
    "  if info != None:\n",
    "    return info.total_visits\n",
    "  return 0\n",
    "\n",
    "def extract_features_for_user(domain_visit_info):\n",
    "  domains = get_most_common_domains()\n",
    "  visits_for_domains = np.array([get_num_visits_for_domain(domain_visit_info, x) for x in domains])\n",
    "  visits_for_domains = np.divide(visits_for_domains, np.sum(visits_for_domains))\n",
    "  return visits_for_domains\n",
    "\n",
    "def extract_features_alldata(data):\n",
    "  output = []\n",
    "  for line in data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    features = extract_features_for_user(domain_visit_info)\n",
    "    output.append(features)\n",
    "  return np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_correct(predicted_labels, actual_labels):\n",
    "  if len(predicted_labels) != len(actual_labels):\n",
    "    raise 'need predicted and actual labels to have same lengths'\n",
    "  total = len(actual_labels)\n",
    "  correct = 0\n",
    "  for p,a in zip(predicted_labels, actual_labels):\n",
    "    if p == a:\n",
    "      correct += 1\n",
    "  return correct / total\n",
    "\n",
    "def test_baseline_classifier():\n",
    "  most_common_label = get_most_common_label()\n",
    "  predictions = [most_common_label for line in test_data]\n",
    "  actual = extract_labels_alldata(test_data)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print('baseline classifier accuracy:', percent_correct)\n",
    "\n",
    "def test_classifier(clf, str=None):\n",
    "  actual = extract_labels_alldata(test_data)\n",
    "  features_test = extract_features_alldata(test_data)\n",
    "  predictions = clf.predict(features_test)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print(str + ' classifier testing accuracy:', percent_correct)\n",
    "\n",
    "def training_error_classifier(clf, str=None):\n",
    "  actual = extract_labels_alldata(training_data)\n",
    "  features_train = extract_features_alldata(training_data)\n",
    "  predictions = clf.predict(features_train)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print(str + ' classifier training accuracy:', round(percent_correct, 2))\n",
    "\n",
    "def to_int_categorical(dt):\n",
    "  # {'easy', 'hard', 'medium', 'nothing'}\n",
    "  cat_dt = []\n",
    "  for item in dt:\n",
    "    if item == 'nothing':\n",
    "      cat_dt.append(0)\n",
    "    elif item == 'easy':\n",
    "      cat_dt.append(1)\n",
    "    elif item == 'medium':\n",
    "      cat_dt.append(2)\n",
    "    else:\n",
    "      cat_dt.append(3)\n",
    "  return np.array(cat_dt)\n",
    "    \n",
    "test_baseline_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF classifier testing accuracy: 0.46153846153846156\n",
      "KNN classifier testing accuracy: 0.48717948717948717\n"
     ]
    }
   ],
   "source": [
    "labels_train = extract_labels_alldata(training_data)\n",
    "features_train = extract_features_alldata(training_data)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, 'RF')\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, p=1)\n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/100\n",
      "154/154 [==============================] - 0s 940us/step - loss: 1.3809 - acc: 0.3052\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - 0s 76us/step - loss: 1.3581 - acc: 0.4351\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - 0s 55us/step - loss: 1.3327 - acc: 0.4481\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - 0s 71us/step - loss: 1.3025 - acc: 0.4481\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - 0s 60us/step - loss: 1.2931 - acc: 0.4481\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - 0s 81us/step - loss: 1.2747 - acc: 0.4481\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - 0s 77us/step - loss: 1.2806 - acc: 0.4481\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - 0s 78us/step - loss: 1.2703 - acc: 0.4481\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - 0s 93us/step - loss: 1.2772 - acc: 0.4481\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - 0s 83us/step - loss: 1.2614 - acc: 0.4481\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - 0s 106us/step - loss: 1.2549 - acc: 0.4481\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - 0s 112us/step - loss: 1.2583 - acc: 0.4481\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - 0s 78us/step - loss: 1.2590 - acc: 0.4481\n",
      "Epoch 14/100\n",
      "154/154 [==============================] - 0s 93us/step - loss: 1.2585 - acc: 0.4481\n",
      "Epoch 15/100\n",
      "154/154 [==============================] - 0s 81us/step - loss: 1.2682 - acc: 0.4481\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - 0s 77us/step - loss: 1.2480 - acc: 0.4481\n",
      "Epoch 17/100\n",
      "154/154 [==============================] - 0s 90us/step - loss: 1.2555 - acc: 0.4481\n",
      "Epoch 18/100\n",
      "154/154 [==============================] - 0s 80us/step - loss: 1.2564 - acc: 0.4545\n",
      "Epoch 19/100\n",
      "154/154 [==============================] - 0s 78us/step - loss: 1.2529 - acc: 0.4481\n",
      "Epoch 20/100\n",
      "154/154 [==============================] - 0s 53us/step - loss: 1.2500 - acc: 0.4481\n",
      "Epoch 21/100\n",
      "154/154 [==============================] - 0s 88us/step - loss: 1.2410 - acc: 0.4481\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - 0s 63us/step - loss: 1.2556 - acc: 0.4481\n",
      "Epoch 23/100\n",
      "154/154 [==============================] - 0s 72us/step - loss: 1.2679 - acc: 0.4481\n",
      "Epoch 24/100\n",
      "154/154 [==============================] - 0s 79us/step - loss: 1.2585 - acc: 0.4481\n",
      "Epoch 25/100\n",
      "154/154 [==============================] - 0s 81us/step - loss: 1.2663 - acc: 0.4481\n",
      "Epoch 26/100\n",
      "154/154 [==============================] - 0s 69us/step - loss: 1.2467 - acc: 0.4481\n",
      "Epoch 27/100\n",
      "154/154 [==============================] - 0s 80us/step - loss: 1.2646 - acc: 0.4481\n",
      "Epoch 28/100\n",
      "154/154 [==============================] - 0s 82us/step - loss: 1.2444 - acc: 0.4481\n",
      "Epoch 29/100\n",
      "154/154 [==============================] - 0s 62us/step - loss: 1.2523 - acc: 0.4481\n",
      "Epoch 30/100\n",
      "154/154 [==============================] - 0s 71us/step - loss: 1.2504 - acc: 0.4481\n",
      "Epoch 31/100\n",
      "154/154 [==============================] - 0s 61us/step - loss: 1.2497 - acc: 0.4481\n",
      "Epoch 32/100\n",
      "154/154 [==============================] - 0s 68us/step - loss: 1.2470 - acc: 0.4481\n",
      "Epoch 33/100\n",
      "154/154 [==============================] - 0s 83us/step - loss: 1.2474 - acc: 0.4545\n",
      "Epoch 34/100\n",
      "154/154 [==============================] - 0s 95us/step - loss: 1.2535 - acc: 0.4481\n",
      "Epoch 35/100\n",
      "154/154 [==============================] - 0s 82us/step - loss: 1.2516 - acc: 0.4481\n",
      "Epoch 36/100\n",
      "154/154 [==============================] - 0s 66us/step - loss: 1.2527 - acc: 0.4481\n",
      "Epoch 37/100\n",
      "154/154 [==============================] - 0s 71us/step - loss: 1.2663 - acc: 0.4481\n",
      "Epoch 38/100\n",
      "154/154 [==============================] - 0s 67us/step - loss: 1.2403 - acc: 0.4481\n",
      "Epoch 39/100\n",
      "154/154 [==============================] - 0s 105us/step - loss: 1.2461 - acc: 0.4481\n",
      "Epoch 40/100\n",
      "154/154 [==============================] - 0s 74us/step - loss: 1.2485 - acc: 0.4416\n",
      "Epoch 41/100\n",
      "154/154 [==============================] - 0s 114us/step - loss: 1.2540 - acc: 0.4481\n",
      "Epoch 42/100\n",
      "154/154 [==============================] - 0s 71us/step - loss: 1.2327 - acc: 0.4481\n",
      "Epoch 43/100\n",
      "154/154 [==============================] - 0s 62us/step - loss: 1.2453 - acc: 0.4481\n",
      "Epoch 44/100\n",
      "154/154 [==============================] - 0s 78us/step - loss: 1.2421 - acc: 0.4481\n",
      "Epoch 45/100\n",
      "154/154 [==============================] - 0s 80us/step - loss: 1.2557 - acc: 0.4481\n",
      "Epoch 46/100\n",
      "154/154 [==============================] - 0s 72us/step - loss: 1.2353 - acc: 0.4481\n",
      "Epoch 47/100\n",
      "154/154 [==============================] - 0s 104us/step - loss: 1.2465 - acc: 0.4481\n",
      "Epoch 48/100\n",
      "154/154 [==============================] - 0s 94us/step - loss: 1.2430 - acc: 0.4481\n",
      "Epoch 49/100\n",
      "154/154 [==============================] - 0s 80us/step - loss: 1.2373 - acc: 0.4481\n",
      "Epoch 50/100\n",
      "154/154 [==============================] - 0s 67us/step - loss: 1.2459 - acc: 0.4416\n",
      "Epoch 51/100\n",
      "154/154 [==============================] - 0s 66us/step - loss: 1.2258 - acc: 0.4481\n",
      "Epoch 52/100\n",
      "154/154 [==============================] - 0s 67us/step - loss: 1.2438 - acc: 0.4481\n",
      "Epoch 53/100\n",
      "154/154 [==============================] - 0s 48us/step - loss: 1.2537 - acc: 0.4545\n",
      "Epoch 54/100\n",
      "154/154 [==============================] - 0s 80us/step - loss: 1.2256 - acc: 0.4481\n",
      "Epoch 55/100\n",
      "154/154 [==============================] - 0s 54us/step - loss: 1.2466 - acc: 0.4416\n",
      "Epoch 56/100\n",
      "154/154 [==============================] - 0s 95us/step - loss: 1.2523 - acc: 0.4481\n",
      "Epoch 57/100\n",
      "154/154 [==============================] - 0s 68us/step - loss: 1.2343 - acc: 0.4545\n",
      "Epoch 58/100\n",
      "154/154 [==============================] - 0s 70us/step - loss: 1.2501 - acc: 0.4545\n",
      "Epoch 59/100\n",
      "154/154 [==============================] - 0s 61us/step - loss: 1.2449 - acc: 0.4545\n",
      "Epoch 60/100\n",
      "154/154 [==============================] - 0s 47us/step - loss: 1.2425 - acc: 0.4481\n",
      "Epoch 61/100\n",
      "154/154 [==============================] - 0s 57us/step - loss: 1.2459 - acc: 0.4416\n",
      "Epoch 62/100\n",
      "154/154 [==============================] - 0s 63us/step - loss: 1.2233 - acc: 0.4481\n",
      "Epoch 63/100\n",
      "154/154 [==============================] - 0s 64us/step - loss: 1.2508 - acc: 0.4481\n",
      "Epoch 64/100\n",
      "154/154 [==============================] - 0s 60us/step - loss: 1.2310 - acc: 0.4481\n",
      "Epoch 65/100\n",
      "154/154 [==============================] - 0s 81us/step - loss: 1.2428 - acc: 0.4545\n",
      "Epoch 66/100\n",
      "154/154 [==============================] - 0s 73us/step - loss: 1.2334 - acc: 0.4481\n",
      "Epoch 67/100\n",
      "154/154 [==============================] - 0s 69us/step - loss: 1.2384 - acc: 0.4481\n",
      "Epoch 68/100\n",
      "154/154 [==============================] - 0s 57us/step - loss: 1.2548 - acc: 0.4416\n",
      "Epoch 69/100\n",
      "154/154 [==============================] - 0s 66us/step - loss: 1.2474 - acc: 0.4610\n",
      "Epoch 70/100\n",
      "154/154 [==============================] - 0s 64us/step - loss: 1.2513 - acc: 0.4481\n",
      "Epoch 71/100\n",
      "154/154 [==============================] - 0s 62us/step - loss: 1.2259 - acc: 0.4481\n",
      "Epoch 72/100\n",
      "154/154 [==============================] - 0s 68us/step - loss: 1.2217 - acc: 0.4610\n",
      "Epoch 73/100\n",
      "154/154 [==============================] - 0s 65us/step - loss: 1.2333 - acc: 0.4481\n",
      "Epoch 74/100\n",
      "154/154 [==============================] - 0s 65us/step - loss: 1.2251 - acc: 0.4675\n",
      "Epoch 75/100\n",
      "154/154 [==============================] - 0s 73us/step - loss: 1.2324 - acc: 0.4545\n",
      "Epoch 76/100\n",
      "154/154 [==============================] - 0s 68us/step - loss: 1.2300 - acc: 0.4481\n",
      "Epoch 77/100\n",
      "154/154 [==============================] - 0s 76us/step - loss: 1.2333 - acc: 0.4610\n",
      "Epoch 78/100\n",
      "154/154 [==============================] - 0s 61us/step - loss: 1.2401 - acc: 0.4481\n",
      "Epoch 79/100\n",
      "154/154 [==============================] - 0s 64us/step - loss: 1.2214 - acc: 0.4545\n",
      "Epoch 80/100\n",
      "154/154 [==============================] - 0s 58us/step - loss: 1.2320 - acc: 0.4675\n",
      "Epoch 81/100\n",
      "154/154 [==============================] - 0s 64us/step - loss: 1.2340 - acc: 0.4675\n",
      "Epoch 82/100\n",
      "154/154 [==============================] - 0s 66us/step - loss: 1.2177 - acc: 0.4545\n",
      "Epoch 83/100\n",
      "154/154 [==============================] - 0s 55us/step - loss: 1.2209 - acc: 0.4740\n",
      "Epoch 84/100\n",
      "154/154 [==============================] - 0s 91us/step - loss: 1.2450 - acc: 0.4416\n",
      "Epoch 85/100\n",
      "154/154 [==============================] - 0s 57us/step - loss: 1.2224 - acc: 0.4805\n",
      "Epoch 86/100\n",
      "154/154 [==============================] - 0s 60us/step - loss: 1.2388 - acc: 0.4351\n",
      "Epoch 87/100\n",
      "154/154 [==============================] - 0s 57us/step - loss: 1.2363 - acc: 0.4416\n",
      "Epoch 88/100\n",
      "154/154 [==============================] - 0s 63us/step - loss: 1.2211 - acc: 0.4610\n",
      "Epoch 89/100\n",
      "154/154 [==============================] - 0s 70us/step - loss: 1.2142 - acc: 0.4610\n",
      "Epoch 90/100\n",
      "154/154 [==============================] - 0s 55us/step - loss: 1.2347 - acc: 0.4416\n",
      "Epoch 91/100\n",
      "154/154 [==============================] - 0s 66us/step - loss: 1.2450 - acc: 0.4481\n",
      "Epoch 92/100\n",
      "154/154 [==============================] - 0s 57us/step - loss: 1.2196 - acc: 0.4740\n",
      "Epoch 93/100\n",
      "154/154 [==============================] - 0s 67us/step - loss: 1.2317 - acc: 0.4610\n",
      "Epoch 94/100\n",
      "154/154 [==============================] - 0s 65us/step - loss: 1.2090 - acc: 0.4610\n",
      "Epoch 95/100\n",
      "154/154 [==============================] - 0s 59us/step - loss: 1.2142 - acc: 0.4481\n",
      "Epoch 96/100\n",
      "154/154 [==============================] - 0s 72us/step - loss: 1.2117 - acc: 0.4545\n",
      "Epoch 97/100\n",
      "154/154 [==============================] - 0s 75us/step - loss: 1.2116 - acc: 0.4545\n",
      "Epoch 98/100\n",
      "154/154 [==============================] - 0s 65us/step - loss: 1.2381 - acc: 0.4351\n",
      "Epoch 99/100\n",
      "154/154 [==============================] - 0s 67us/step - loss: 1.2013 - acc: 0.4675\n",
      "Epoch 100/100\n",
      "154/154 [==============================] - 0s 66us/step - loss: 1.2213 - acc: 0.4481\n",
      "39/39 [==============================] - 0s 958us/step\n",
      "[[0.12318765 0.4499732  0.28594592 0.14089328]\n",
      " [0.12705004 0.3757309  0.3617288  0.13549024]\n",
      " [0.14537771 0.51154923 0.21668601 0.12638706]\n",
      " [0.13326488 0.43772033 0.28760827 0.14140649]\n",
      " [0.1325988  0.48823342 0.23347294 0.14569484]\n",
      " [0.14305243 0.49087533 0.24089581 0.12517649]\n",
      " [0.13953753 0.4639587  0.25187477 0.14462905]\n",
      " [0.14092146 0.40827096 0.29428467 0.15652286]\n",
      " [0.13725467 0.41670635 0.30516875 0.14087024]\n",
      " [0.12516199 0.42814502 0.3127718  0.13392124]\n",
      " [0.1386173  0.42967907 0.29708406 0.13461964]\n",
      " [0.12710801 0.43430257 0.30362773 0.13496171]\n",
      " [0.11873891 0.45084095 0.28889894 0.1415212 ]\n",
      " [0.13186541 0.4789302  0.2556527  0.1335517 ]\n",
      " [0.11913513 0.39714473 0.35618383 0.12753631]\n",
      " [0.13973485 0.37713885 0.35260162 0.13052471]\n",
      " [0.13276169 0.4175003  0.3159978  0.13374017]\n",
      " [0.1416341  0.5198824  0.20656037 0.13192312]\n",
      " [0.13835393 0.38710678 0.34064782 0.1338915 ]\n",
      " [0.13069145 0.46221933 0.27170578 0.13538344]\n",
      " [0.12094074 0.41548488 0.3239315  0.13964295]\n",
      " [0.14250538 0.38763818 0.33037984 0.1394766 ]\n",
      " [0.15560526 0.43196654 0.26749462 0.14493363]\n",
      " [0.13967049 0.36771458 0.36061642 0.13199852]\n",
      " [0.11852647 0.40290362 0.34554994 0.13301988]\n",
      " [0.14279777 0.5547742  0.17788385 0.12454417]\n",
      " [0.1421655  0.4178197  0.30522704 0.1347878 ]\n",
      " [0.1303062  0.45156842 0.27991456 0.13821077]\n",
      " [0.14230412 0.44577253 0.26981288 0.14211051]\n",
      " [0.12209563 0.43804288 0.31480736 0.12505408]\n",
      " [0.14108388 0.48441127 0.24670444 0.12780046]\n",
      " [0.13851082 0.42849702 0.29598987 0.13700238]\n",
      " [0.12988093 0.4771703  0.25061125 0.14233747]\n",
      " [0.13269135 0.4812358  0.2192237  0.1668492 ]\n",
      " [0.12845585 0.45272672 0.2838734  0.13494399]\n",
      " [0.14056681 0.46856564 0.2574762  0.13339129]\n",
      " [0.12807368 0.4175989  0.31745508 0.13687228]\n",
      " [0.16108043 0.43135092 0.2609221  0.14664654]\n",
      " [0.16487902 0.4964869  0.22280419 0.11582994]]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[1.2032574751438239, 0.5128205158771613]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "labels_test = to_categorical(to_int_categorical(extract_labels_alldata(test_data)), num_classes=4)\n",
    "features_test = extract_features_alldata(test_data)\n",
    "\n",
    "labels_train = to_categorical(to_int_categorical(extract_labels_alldata(training_data)), num_classes=4)\n",
    "features_train = extract_features_alldata(training_data)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(features_train, labels_train,\n",
    "          epochs=100,\n",
    "          batch_size=32)\n",
    "score = model.evaluate(features_test, labels_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(features_test, batch_size=32)\n",
    "\n",
    "print(predictions)\n",
    "print(labels_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
