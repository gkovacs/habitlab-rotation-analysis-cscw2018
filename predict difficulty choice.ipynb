{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import lzstring\n",
    "from collections import namedtuple, Counter\n",
    "import json\n",
    "from memoize import memoize\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [num unique urls, num unique urls typed, total visits, total typed, first visit time, last visit time]\n",
    "domaininfo = namedtuple('domaininfo', ['num_unique_urls', 'num_unique_urls_typed', 'total_visits', 'total_typed', 'first_visit_time', 'last_visit_time'])\n",
    "\n",
    "decompressFromEncodedURIComponent = lzstring.LZString().decompressFromEncodedURIComponent\n",
    "\n",
    "#filepath = 'difficultyselectionexp_may31_11am.csv'\n",
    "filepath = 'difficultyselectionexp_june6_3pm.csv'\n",
    "reader = csv.DictReader(open(filepath))\n",
    "\n",
    "def extract_domain_visit_info(domain_visit_info_compressed):\n",
    "  domain_visit_info = json.loads(decompressFromEncodedURIComponent(domain_visit_info_compressed))\n",
    "  output = {}\n",
    "  for k,v in domain_visit_info.items():\n",
    "    linedata = domaininfo(*v)\n",
    "    output[k] = linedata\n",
    "  return output\n",
    "\n",
    "alldata = []\n",
    "\n",
    "for alldata_item in reader:\n",
    "  if alldata_item['selected_difficulty'] not in ['nothing', 'easy', 'medium', 'hard']:\n",
    "    continue\n",
    "  if alldata_item['domain_visit_info_compressed'] == None or len(alldata_item['domain_visit_info_compressed']) == 0:\n",
    "    continue\n",
    "  alldata_item['domain_visit_info'] = extract_domain_visit_info(alldata_item['domain_visit_info_compressed'])\n",
    "  alldata.append(alldata_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "training_data = alldata[:round(len(alldata)*0.8)]\n",
    "test_data = alldata[round(len(alldata)*0.8):]\n",
    "print(len(training_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_alldata(data):\n",
    "  return np.array([line['selected_difficulty'] for line in data])\n",
    "\n",
    "@memoize\n",
    "def get_most_common_label():\n",
    "  label_to_count = Counter()\n",
    "  for line in training_data:\n",
    "    label = line['selected_difficulty']\n",
    "    label_to_count[label] += 1\n",
    "  sorted_by_count = sorted(label_to_count.items(), key=lambda x: x[1], reverse=True)\n",
    "  return sorted_by_count[0][0]\n",
    "\n",
    "@memoize\n",
    "def get_most_visited_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += info.total_visits\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "@memoize\n",
    "def get_most_common_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += 1\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "def get_num_visits_for_domain(domain_visit_info, domain):\n",
    "  info = domain_visit_info.get(domain, None)\n",
    "  if info != None:\n",
    "    return info.total_visits\n",
    "  return 0\n",
    "\n",
    "def extract_features_for_user(domain_visit_info):\n",
    "  domains = get_most_common_domains()\n",
    "  visits_for_domains = np.array([get_num_visits_for_domain(domain_visit_info, x) for x in domains])\n",
    "  visits_for_domains = np.divide(visits_for_domains, np.sum(visits_for_domains))\n",
    "  return visits_for_domains\n",
    "\n",
    "def extract_features_alldata(data):\n",
    "  output = []\n",
    "  for line in data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    features = extract_features_for_user(domain_visit_info)\n",
    "    output.append(features)\n",
    "  return np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline classifier accuracy: 0.5945945945945946\n"
     ]
    }
   ],
   "source": [
    "def get_percent_correct(predicted_labels, actual_labels):\n",
    "  if len(predicted_labels) != len(actual_labels):\n",
    "    raise 'need predicted and actual labels to have same lengths'\n",
    "  total = len(actual_labels)\n",
    "  correct = 0\n",
    "  for p,a in zip(predicted_labels, actual_labels):\n",
    "    if p == a:\n",
    "      correct += 1\n",
    "  return correct / total\n",
    "\n",
    "def test_baseline_classifier():\n",
    "  most_common_label = get_most_common_label()\n",
    "  predictions = [most_common_label for line in test_data]\n",
    "  actual = extract_labels_alldata(test_data)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print('baseline classifier accuracy:', percent_correct)\n",
    "\n",
    "def test_classifier(clf, str=None):\n",
    "  actual = extract_labels_alldata(test_data)\n",
    "  features_test = extract_features_alldata(test_data)\n",
    "  predictions = clf.predict(features_test)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print(str + ' classifier testing accuracy:', percent_correct)\n",
    "\n",
    "def training_error_classifier(clf, str=None):\n",
    "  actual = extract_labels_alldata(training_data)\n",
    "  features_train = extract_features_alldata(training_data)\n",
    "  predictions = clf.predict(features_train)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print(str + ' classifier training accuracy:', round(percent_correct, 2))\n",
    "\n",
    "def to_int_categorical(dt):\n",
    "  # {'easy', 'hard', 'medium', 'nothing'}\n",
    "  cat_dt = []\n",
    "  for item in dt:\n",
    "    if item == 'nothing':\n",
    "      cat_dt.append(0)\n",
    "    elif item == 'easy':\n",
    "      cat_dt.append(1)\n",
    "    elif item == 'medium':\n",
    "      cat_dt.append(2)\n",
    "    else:\n",
    "      cat_dt.append(3)\n",
    "  return np.array(cat_dt)\n",
    "    \n",
    "test_baseline_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF classifier testing accuracy: 0.3783783783783784\n",
      "KNN classifier testing accuracy: 0.5135135135135135\n"
     ]
    }
   ],
   "source": [
    "labels_train = extract_labels_alldata(training_data)\n",
    "features_train = extract_features_alldata(training_data)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, 'RF')\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, p=1)\n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 0s 810us/step - loss: 1.3933 - acc: 0.2534\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 0s 69us/step - loss: 1.3641 - acc: 0.4178\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 0s 69us/step - loss: 1.3470 - acc: 0.4247\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 0s 81us/step - loss: 1.3228 - acc: 0.4041\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 0s 87us/step - loss: 1.3016 - acc: 0.4315\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 0s 92us/step - loss: 1.2879 - acc: 0.4315\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 0s 67us/step - loss: 1.2822 - acc: 0.4452\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 0s 104us/step - loss: 1.2922 - acc: 0.4315\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 0s 91us/step - loss: 1.2689 - acc: 0.4384\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 0s 76us/step - loss: 1.2697 - acc: 0.4315\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 0s 68us/step - loss: 1.2861 - acc: 0.4315\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 0s 81us/step - loss: 1.2798 - acc: 0.4384\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 0s 84us/step - loss: 1.2880 - acc: 0.4384\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 0s 64us/step - loss: 1.2812 - acc: 0.4315\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 0s 68us/step - loss: 1.2758 - acc: 0.4315\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 0s 96us/step - loss: 1.2711 - acc: 0.4315\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 0s 92us/step - loss: 1.2684 - acc: 0.4384\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 0s 90us/step - loss: 1.2624 - acc: 0.4315\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 0s 88us/step - loss: 1.2562 - acc: 0.4315\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 0s 88us/step - loss: 1.2724 - acc: 0.4315\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 0s 74us/step - loss: 1.2750 - acc: 0.4247\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 0s 86us/step - loss: 1.2728 - acc: 0.4384\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 0s 94us/step - loss: 1.2617 - acc: 0.4384\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 0s 80us/step - loss: 1.2728 - acc: 0.4247\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 0s 76us/step - loss: 1.2750 - acc: 0.4247\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 0s 79us/step - loss: 1.2725 - acc: 0.4384\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 0s 93us/step - loss: 1.2616 - acc: 0.4315\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 0s 71us/step - loss: 1.2693 - acc: 0.4384\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 0s 78us/step - loss: 1.2541 - acc: 0.4384\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 0s 79us/step - loss: 1.2679 - acc: 0.4384\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 0s 90us/step - loss: 1.2655 - acc: 0.4315\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 0s 83us/step - loss: 1.2712 - acc: 0.4384\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 0s 73us/step - loss: 1.2462 - acc: 0.4384\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 0s 88us/step - loss: 1.2600 - acc: 0.4384\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 0s 93us/step - loss: 1.2629 - acc: 0.4178\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 0s 74us/step - loss: 1.2498 - acc: 0.4384\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 0s 73us/step - loss: 1.2493 - acc: 0.4452\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 0s 83us/step - loss: 1.2726 - acc: 0.4247\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 0s 94us/step - loss: 1.2368 - acc: 0.4452\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 0s 66us/step - loss: 1.2708 - acc: 0.4110\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 0s 61us/step - loss: 1.2688 - acc: 0.4452\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 0s 102us/step - loss: 1.2550 - acc: 0.4452\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 0s 100us/step - loss: 1.2751 - acc: 0.4521\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 0s 83us/step - loss: 1.2566 - acc: 0.4521\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 0s 76us/step - loss: 1.2723 - acc: 0.4315\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 0s 99us/step - loss: 1.2553 - acc: 0.4247\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 0s 99us/step - loss: 1.2557 - acc: 0.4384\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 0s 105us/step - loss: 1.2544 - acc: 0.4384\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 0s 80us/step - loss: 1.2686 - acc: 0.4384\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 0s 100us/step - loss: 1.2483 - acc: 0.4726\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 0s 62us/step - loss: 1.2390 - acc: 0.4521\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 0s 83us/step - loss: 1.2632 - acc: 0.4384\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 0s 101us/step - loss: 1.2537 - acc: 0.4384\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 0s 97us/step - loss: 1.2604 - acc: 0.4452\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 0s 80us/step - loss: 1.2515 - acc: 0.4589\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 0s 77us/step - loss: 1.2515 - acc: 0.4452\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 0s 76us/step - loss: 1.2393 - acc: 0.4521\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 0s 73us/step - loss: 1.2601 - acc: 0.4384\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 0s 88us/step - loss: 1.2596 - acc: 0.4452\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 0s 85us/step - loss: 1.2756 - acc: 0.3836\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 0s 71us/step - loss: 1.2510 - acc: 0.4315\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 0s 67us/step - loss: 1.2438 - acc: 0.4247\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 0s 78us/step - loss: 1.2432 - acc: 0.4384\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 0s 61us/step - loss: 1.2485 - acc: 0.4384\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 0s 73us/step - loss: 1.2494 - acc: 0.4110\n",
      "Epoch 66/100\n",
      "146/146 [==============================] - 0s 84us/step - loss: 1.2579 - acc: 0.4384\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 0s 97us/step - loss: 1.2568 - acc: 0.4315\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 0s 67us/step - loss: 1.2290 - acc: 0.4521\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 0s 72us/step - loss: 1.2526 - acc: 0.4384\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 0s 73us/step - loss: 1.2572 - acc: 0.4452\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 0s 75us/step - loss: 1.2457 - acc: 0.4452\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 0s 82us/step - loss: 1.2328 - acc: 0.4726\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 0s 68us/step - loss: 1.2368 - acc: 0.4726\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 0s 99us/step - loss: 1.2352 - acc: 0.4521\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 0s 61us/step - loss: 1.2449 - acc: 0.4384\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 0s 78us/step - loss: 1.2408 - acc: 0.4658\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 0s 72us/step - loss: 1.2604 - acc: 0.4247\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 0s 69us/step - loss: 1.2267 - acc: 0.4521\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 0s 89us/step - loss: 1.2433 - acc: 0.4178\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 0s 73us/step - loss: 1.2355 - acc: 0.4452\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 0s 66us/step - loss: 1.2585 - acc: 0.4247\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 0s 70us/step - loss: 1.2507 - acc: 0.4658\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 0s 63us/step - loss: 1.2370 - acc: 0.4247\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 0s 59us/step - loss: 1.2359 - acc: 0.4726\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 0s 88us/step - loss: 1.2398 - acc: 0.4384\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 0s 61us/step - loss: 1.2405 - acc: 0.4521\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 0s 75us/step - loss: 1.2341 - acc: 0.4384\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 0s 82us/step - loss: 1.2243 - acc: 0.4589\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 0s 68us/step - loss: 1.2272 - acc: 0.4452\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 0s 70us/step - loss: 1.2382 - acc: 0.4589\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 0s 73us/step - loss: 1.2449 - acc: 0.4589\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 0s 77us/step - loss: 1.2361 - acc: 0.4589\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 0s 67us/step - loss: 1.2220 - acc: 0.4658\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 0s 77us/step - loss: 1.2307 - acc: 0.4863\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 0s 74us/step - loss: 1.2457 - acc: 0.4384\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 0s 71us/step - loss: 1.2334 - acc: 0.4521\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 0s 76us/step - loss: 1.2394 - acc: 0.4658\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 0s 69us/step - loss: 1.2279 - acc: 0.4795\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 0s 70us/step - loss: 1.2283 - acc: 0.4521\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 0s 71us/step - loss: 1.2210 - acc: 0.4521\n",
      "37/37 [==============================] - 0s 986us/step\n",
      "[[0.13082817 0.39454156 0.3227495  0.1518807 ]\n",
      " [0.14010698 0.41938275 0.28863302 0.15187727]\n",
      " [0.1463737  0.43314803 0.28082392 0.13965441]\n",
      " [0.14039281 0.4779375  0.24397698 0.13769276]\n",
      " [0.10879043 0.4218053  0.3308282  0.13857612]\n",
      " [0.14306375 0.41156814 0.3028608  0.14250737]\n",
      " [0.16495907 0.41907054 0.26895273 0.14701769]\n",
      " [0.11311682 0.45184353 0.293402   0.14163765]\n",
      " [0.11313765 0.41214126 0.33643097 0.13829015]\n",
      " [0.15857889 0.49527374 0.21738236 0.12876497]\n",
      " [0.15944335 0.44952103 0.24941438 0.14162125]\n",
      " [0.1282581  0.50347435 0.23173183 0.1365357 ]\n",
      " [0.14870417 0.4625523  0.25263387 0.1361096 ]\n",
      " [0.13569038 0.48192278 0.25066733 0.13171951]\n",
      " [0.13642554 0.4270242  0.2943543  0.14219597]\n",
      " [0.13450447 0.41833666 0.30359825 0.1435607 ]\n",
      " [0.11511401 0.45081612 0.29636186 0.13770802]\n",
      " [0.12603167 0.43365067 0.2969811  0.14333652]\n",
      " [0.14112225 0.4747914  0.24705504 0.13703129]\n",
      " [0.1321619  0.36005294 0.3588041  0.1489811 ]\n",
      " [0.12567902 0.4244197  0.30922407 0.1406772 ]\n",
      " [0.14667751 0.49240825 0.22288598 0.13802823]\n",
      " [0.11941757 0.4250737  0.3137144  0.14179431]\n",
      " [0.14190058 0.38743913 0.3219454  0.14871491]\n",
      " [0.18218958 0.45655638 0.2329955  0.12825859]\n",
      " [0.13048768 0.36229932 0.35675752 0.15045553]\n",
      " [0.09691048 0.40814233 0.36187854 0.13306871]\n",
      " [0.15038309 0.51691175 0.19915754 0.13354762]\n",
      " [0.13867114 0.4145284  0.29874113 0.14805931]\n",
      " [0.1412254  0.44788843 0.27116752 0.13971865]\n",
      " [0.15062258 0.48274267 0.24064417 0.12599058]\n",
      " [0.14259249 0.41289425 0.29714605 0.14736724]\n",
      " [0.12609527 0.4798691  0.25322625 0.14080943]\n",
      " [0.1561652  0.47474498 0.22492705 0.14416276]\n",
      " [0.14305702 0.46259704 0.25724047 0.1371054 ]\n",
      " [0.14768913 0.45219114 0.25728583 0.14283387]\n",
      " [0.16274114 0.4972686  0.22185497 0.11813524]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[1.1503775925249666, 0.5945945962055309]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "labels_test = to_categorical(to_int_categorical(extract_labels_alldata(test_data)), num_classes=4)\n",
    "features_test = extract_features_alldata(test_data)\n",
    "\n",
    "labels_train = to_categorical(to_int_categorical(extract_labels_alldata(training_data)), num_classes=4)\n",
    "features_train = extract_features_alldata(training_data)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(features_train, labels_train,\n",
    "          epochs=100,\n",
    "          batch_size=32)\n",
    "score = model.evaluate(features_test, labels_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(features_test, batch_size=32)\n",
    "\n",
    "print(predictions)\n",
    "print(labels_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
