{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import lzstring\n",
    "from collections import namedtuple, Counter\n",
    "import json\n",
    "from memoize import memoize\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm \n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# [num unique urls, num unique urls typed, total visits, total typed, first visit time, last visit time]\n",
    "domaininfo = namedtuple('domaininfo', ['num_unique_urls', 'num_unique_urls_typed', 'total_visits', 'total_typed', 'first_visit_time', 'last_visit_time'])\n",
    "\n",
    "decompressFromEncodedURIComponent = lzstring.LZString().decompressFromEncodedURIComponent\n",
    "\n",
    "#filepath = 'difficultyselectionexp_may31_11am.csv'\n",
    "filepath = 'data/JUL31.csv'\n",
    "reader = csv.DictReader(open(filepath))\n",
    "\n",
    "def extract_domain_visit_info(domain_visit_info_compressed):\n",
    "  domain_visit_info = json.loads(decompressFromEncodedURIComponent(domain_visit_info_compressed))\n",
    "  output = {}\n",
    "  for k,v in domain_visit_info.items():\n",
    "    linedata = domaininfo(*v)\n",
    "    output[k] = linedata\n",
    "  return output\n",
    "\n",
    "alldata = []\n",
    "\n",
    "for alldata_item in reader:\n",
    "  if alldata_item['selected_difficulty'] not in ['nothing', 'easy', 'medium', 'hard']:\n",
    "    continue\n",
    "  if alldata_item['domain_visit_info_compressed'] == None or len(alldata_item['domain_visit_info_compressed']) == 0:\n",
    "    continue\n",
    "  alldata_item['domain_visit_info'] = extract_domain_visit_info(alldata_item['domain_visit_info_compressed'])\n",
    "  alldata.append(alldata_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "#np.random.shuffle(alldata)\n",
    "training_data = alldata[:round(len(alldata)*0.8)]\n",
    "test_data = alldata[round(len(alldata)*0.8):]\n",
    "print(len(training_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_alldata(data):\n",
    "  return np.array([line['selected_difficulty'] for line in data])\n",
    "\n",
    "@memoize\n",
    "def get_most_common_label():\n",
    "  label_to_count = Counter()\n",
    "  for line in training_data:\n",
    "    label = line['selected_difficulty']\n",
    "    label_to_count[label] += 1\n",
    "  sorted_by_count = sorted(label_to_count.items(), key=lambda x: x[1], reverse=True)\n",
    "  return sorted_by_count[0][0]\n",
    "\n",
    "@memoize\n",
    "def get_most_visited_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += info.total_visits\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "cnt = 0\n",
    "@memoize\n",
    "def get_most_common_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += 1\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits[:100]]\n",
    "\n",
    "def get_all_domains():\n",
    "  domain_to_num_visits = Counter()\n",
    "  for line in training_data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    for domain,info in domain_visit_info.items():\n",
    "      domain_to_num_visits[domain] += 1\n",
    "  sorted_by_num_visits = sorted(domain_to_num_visits.items(), key=lambda x: x[1], reverse=True)\n",
    "  return [x[0] for x in sorted_by_num_visits]\n",
    "\n",
    "\n",
    "\n",
    "def get_num_visits_for_domain(domain_visit_info, domain):\n",
    "  info = domain_visit_info.get(domain, None)\n",
    "  if info != None:\n",
    "    return info.total_visits\n",
    "  return 0\n",
    "\n",
    "def get_productivity():\n",
    "    with open ('domain_to_productivity.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "def get_category():\n",
    "    with open ('domain_to_category.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data\n",
    "\n",
    "domain_to_productivity = get_productivity()\n",
    "domain_to_category = get_category()\n",
    "#number of Categories = 60 \n",
    "\n",
    "def get_category_domain (domain):\n",
    "    if domain in domain_to_category.keys():\n",
    "        return domain_to_category[domain]\n",
    "    s = domain.split('.')\n",
    "    domain2 = \"www.\" + s[len(s) - 2] + \".com\"\n",
    "    if domain2 in domain_to_category.keys():\n",
    "        return domain_to_category[domain2]\n",
    "    return None\n",
    "\n",
    "def extract_features_for_user(domain_visit_info):\n",
    "    cnt = 0\n",
    "    domains = get_all_domains()\n",
    "    final_features = []\n",
    "    category_to_int = {}\n",
    "    ind = 0\n",
    "    for x in domain_to_category.keys():\n",
    "        category_to_int[domain_to_category[x]] = 0\n",
    "    \n",
    "    for x in category_to_int.keys():\n",
    "        final_features.append(0)\n",
    "        category_to_int[x] = ind\n",
    "        ind += 1\n",
    " \n",
    "    for x in domains:\n",
    "        if get_category_domain(x) != None:\n",
    "            final_features[category_to_int[get_category_domain(x)]] = get_num_visits_for_domain(domain_visit_info, x)\n",
    "    \n",
    "  #np.array([get_num_visits_for_domain(domain_visit_info, x) for x in domains])\n",
    "  #if np.sum(final_features) >= 1:\n",
    "  #  final_features = np.divide(final_features, np.sum(final_features))\n",
    "    return final_features\n",
    "\n",
    "def extract_features_alldata(data):\n",
    "  output = []\n",
    "  for line in data:\n",
    "    domain_visit_info = line['domain_visit_info']\n",
    "    features = extract_features_for_user(domain_visit_info)\n",
    "    output.append(features)\n",
    "  return np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline classifier accuracy: 0.5082872928176796\n"
     ]
    }
   ],
   "source": [
    "def get_percent_correct(predicted_labels, actual_labels):\n",
    "  if len(predicted_labels) != len(actual_labels):\n",
    "    raise 'need predicted and actual labels to have same lengths'\n",
    "  total = len(actual_labels)\n",
    "  correct = 0\n",
    "  for p,a in zip(predicted_labels, actual_labels):\n",
    "    if p == a:\n",
    "      correct += 1\n",
    "  return correct / total\n",
    "\n",
    "def test_baseline_classifier():\n",
    "  most_common_label = get_most_common_label()\n",
    "  predictions = [most_common_label for line in test_data]\n",
    "  actual = extract_labels_alldata(test_data)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print('baseline classifier accuracy:', percent_correct)\n",
    "\n",
    "def test_classifier(clf,features_test, actual, str=None):\n",
    "  #actual = extract_labels_alldata(test_data)\n",
    "  #features_test = extract_features_alldata(test_data)\n",
    "  predictions = clf.predict(features_test)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  return percent_correct\n",
    "\n",
    "def training_error_classifier(clf, str=None):\n",
    "  actual = extract_labels_alldata(training_data)\n",
    "  features_train = extract_features_alldata(training_data)\n",
    "  predictions = clf.predict(features_train)\n",
    "  percent_correct = get_percent_correct(predictions, actual)\n",
    "  print(str + ' classifier training accuracy:', round(percent_correct, 2))\n",
    "\n",
    "def to_int_categorical(dt):\n",
    "  # {'easy', 'hard', 'medium', 'nothing'}\n",
    "  cat_dt = []\n",
    "  for item in dt:\n",
    "    if item == 'nothing':\n",
    "      cat_dt.append(0)\n",
    "    elif item == 'easy':\n",
    "      cat_dt.append(1)\n",
    "    elif item == 'medium':\n",
    "      cat_dt.append(2)\n",
    "    else:\n",
    "      cat_dt.append(3)\n",
    "  return np.array(cat_dt)\n",
    "    \n",
    "test_baseline_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "\n",
    "labels_test = extract_labels_alldata(test_data)\n",
    "features_test = extract_features_alldata(test_data)\n",
    "\n",
    "labels_train = extract_labels_alldata(training_data)\n",
    "features_train = extract_features_alldata(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier():\n",
    "    clf = KNeighborsClassifier(n_neighbors=3, p=1)\n",
    "    clf2 = svm.SVC(decision_function_shape = \"ovo\")\n",
    "    clf.fit(features_train, labels_train)\n",
    "    clf2.fit(features_train, labels_train)\n",
    "    print (test_classifier(clf, features_test, labels_test))\n",
    "    print (test_classifier(clf2, features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(726, 69)\n",
      "medium\n",
      "0.47513812154696133\n",
      "0.5082872928176796\n"
     ]
    }
   ],
   "source": [
    "print (features_train.shape)\n",
    "print (labels_train[0])\n",
    "classifier()\n",
    "\n",
    "labels_test2 = labels_test\n",
    "features_test2 = features_test \n",
    "labels_train2 = labels_train\n",
    "features_test2 = features_test \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
